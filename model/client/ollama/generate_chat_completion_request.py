"""
    Assistance Backbone for the assistance system developed as part of the VerDatAs project
    Copyright (C) 2022-2024 TU Dresden (Robert Schmidt, Sebastian Kucharski)

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.
"""

"""
    Ollama API

    API Spec for Ollama API. Please see https://github.com/jmorganca/ollama/blob/main/docs/api.md for more details.

    The version of the OpenAPI document: 0.1.9
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501

from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from typing import Any, ClassVar, Dict, List, Optional

from pydantic import BaseModel, Field, StrictBool, StrictInt, StrictStr

from model.client.ollama.message import Message
from model.client.ollama.request_options import RequestOptions
from model.client.ollama.response_format import ResponseFormat

try:
    from typing import Self
except ImportError:
    from typing_extensions import Self


class GenerateChatCompletionRequest(BaseModel):
    """
    Request class for the chat endpoint.
    """  # noqa: E501
    model: StrictStr = Field(
        description="The model name.  Model names follow a `model:tag` format. Some examples are `orca-mini:3b-q4_1` and `llama2:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version. ")
    messages: List[Message] = Field(description="The messages of the chat, this can be used to keep a chat memory")
    format: Optional[ResponseFormat] = None
    options: Optional[RequestOptions] = None
    stream: Optional[StrictBool] = Field(default=False,
                                         description="If `false` the response will be returned as a single response object, otherwise the response will be streamed as a series of objects. ")
    keep_alive: Optional[StrictInt] = Field(default=None,
                                            description="How long (in minutes) to keep the model loaded in memory.  - If set to a positive duration (e.g. 20), the model will stay loaded for the provided duration. - If set to a negative duration (e.g. -1), the model will stay loaded indefinitely. - If set to 0, the model will be unloaded immediately once finished. - If not set, the model will stay loaded for 5 minutes by default ")
    __properties: ClassVar[List[str]] = ["model", "messages", "format", "options", "stream", "keep_alive"]

    model_config = {
        "populate_by_name": True,
        "validate_assignment": True,
        "protected_namespaces": (),
    }

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Self:
        """Create an instance of GenerateChatCompletionRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        _dict = self.model_dump(
            by_alias=True,
            exclude={
            },
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in messages (list)
        _items = []
        if self.messages:
            for _item in self.messages:
                if _item:
                    _items.append(_item.to_dict())
            _dict['messages'] = _items
        # override the default output from pydantic by calling `to_dict()` of options
        if self.options:
            _dict['options'] = self.options.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Dict) -> Self:
        """Create an instance of GenerateChatCompletionRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "model": obj.get("model"),
            "messages": [Message.from_dict(_item) for _item in obj.get("messages")] if obj.get(
                "messages") is not None else None,
            "format": obj.get("format"),
            "options": RequestOptions.from_dict(obj.get("options")) if obj.get("options") is not None else None,
            "stream": obj.get("stream") if obj.get("stream") is not None else False,
            "keep_alive": obj.get("keep_alive")
        })
        return _obj
